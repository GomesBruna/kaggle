{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "426ff61f-037e-ee11-cc38-05f46d272b19"
   },
   "source": [
    "Make my hand dirty with Quora challenge.This is my first competition.Please suggest me to do better and it if helpful then don't forget to up vote and leave you feedback  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "9330326e-3de2-026d-168c-462e111ec625",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Initial Packages\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from subprocess import check_output\n",
    "import gensim\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re \n",
    "from collections import namedtuple\n",
    "import multiprocessing\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stopwords = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39e85dac-c0ca-f3b6-b1d4-dc28fd247da9"
   },
   "source": [
    "**Data Analysis and Natural Language processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "425f14f1-f0fd-f83a-c960-203c500da2f2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "be8486b3-6739-21b2-d250-59ebd3fd8aec",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_set1 = df_train[[\"qid1\",\"question1\"]]\n",
    "df_train_set2 = df_train[[\"qid2\",\"question2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "d23772d5-e04b-9c27-08bb-ebcd1a980c03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_set_1 : (404290, 2)\n",
      "df_train_set_2 : (404290, 2)\n",
      "df_train_set : (808580, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train_set1.columns = [\"qid\",\"question\"]\n",
    "df_train_set2.columns =[\"qid\",\"question\"]\n",
    "df_train_set = pd.concat([df_train_set1,df_train_set2],axis=0)\n",
    "print(\"df_train_set_1 :\",df_train_set1.shape)\n",
    "print(\"df_train_set_2 :\",df_train_set1.shape)\n",
    "print(\"df_train_set :\",df_train_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "7807c022-9e81-aa25-4cf9-a087cde45733",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Language Processing\n",
    "def get_processed_text(text=\"\"):\n",
    "    \"\"\"\n",
    "    Remove stopword,lemmatizing the words and remove special character to get important content\n",
    "    \"\"\"\n",
    "    clean_text = re.sub('[^a-zA-Z0-9 \\n\\.]', ' ', text)\n",
    "    tokens = tokenizer.tokenize(clean_text)\n",
    "    tokens = [lemmatizer.lemmatize(token.lower().strip()) for token in tokens\n",
    "              if token not in stopwords and len(token) >= 2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "46569a8e-dbab-3871-7883-a3c033ba9af9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text :  What is the best phone to buy below 15k\n",
      "Processed Text :  what best phone buy 15k\n"
     ]
    }
   ],
   "source": [
    "text = \"What is the best phone to buy below 15k\"\n",
    "print (\"Original Text : \",text)\n",
    "processed_text = \" \".join(get_processed_text(text))\n",
    "print (\"Processed Text : \",processed_text) #Remove special character(?),english stop words(is,the,by,to,in) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "45731233-f092-5332-7c9a-7e7058feaa63",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Process and clean up traing set\n",
    "alldocuments = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')       \n",
    "keywords = []\n",
    "for index,record in df_train_set[0:100].iterrows():\n",
    "    qid = str(record[\"qid\"])\n",
    "    question = str(record[\"question\"])\n",
    "    tokens = get_processed_text(question)\n",
    "    words = tokens\n",
    "    words_text = \" \".join(words)\n",
    "    words = gensim.utils.simple_preprocess(words_text)\n",
    "    tags = [qid]\n",
    "    alldocuments.append(analyzedDocument(words, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "3fc5dc0a-8a87-bd93-fdf0-138bd7dd6b0f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_save_doc2vec_model(alldocuments,document_model=\"model1\",m_iter=100,m_min_count=2,m_size=100,m_window=5):\n",
    "            print (\"Start Time : %s\" %(str(datetime.datetime.now())))\n",
    "            #Train Model\n",
    "            cores = multiprocessing.cpu_count()\n",
    "            abs_path = os.getcwd()\n",
    "            saved_model_name = \"doc_2_vec_%s\" %(document_model)\n",
    "            doc_vec_file = \"%s\" %(saved_model_name)\n",
    "            if document_model == \"model1\":\n",
    "                # PV-DBOW \n",
    "                model_1 = gensim.models.Doc2Vec(alldocuments,dm=0,workers=cores,size=m_size, window=m_window,min_count=m_min_count,iter=m_iter,dbow_words=1)\n",
    "                model_1.save(\"%s\" %(doc_vec_file))\n",
    "                print (\"model training completed : %s\" %(doc_vec_file))\n",
    "            elif document_model == \"model2\":\n",
    "                # PV-DBOW \n",
    "                model_2 = gensim.models.Doc2Vec(alldocuments,dm=0,workers=cores,size=m_size, window=m_window,min_count=m_min_count,iter=m_iter,dbow_words=0)\n",
    "                model_2.save(\"%s\" %(doc_vec_file))\n",
    "                print (\"model training completed : %s\" %(doc_vec_file))\n",
    "            elif document_model == \"model3\":\n",
    "                # PV-DM w/average\n",
    "                model_3 = gensim.models.Doc2Vec(alldocuments,dm=1, dm_mean=1,size=m_size, window=m_window,min_count=m_min_count,iter=m_iter)\n",
    "                model_3.save(\"%s\" %(doc_vec_file))\n",
    "                print (\"model training completed : %s\" %(doc_vec_file))\n",
    "\n",
    "            elif document_model == \"model4\":\n",
    "                # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "                model_4 = gensim.models.Doc2Vec(alldocuments,dm=1, dm_concat=1,workers=cores, size=m_size, window=m_window,min_count=m_min_count,iter=m_iter)\n",
    "                model_4.save(\"%s\" %(doc_vec_file))\n",
    "                print (\"model training completed : %s\" %(doc_vec_file))\n",
    "            print (\"Record count %s\" %len(alldocuments))\n",
    "            print (\"End Time %s\" %(str(datetime.datetime.now())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "0191d569-1dc5-c4f5-d698-5b162193d486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time : 2017-06-01 22:46:01.810935\n",
      "model training completed : doc_2_vec_model1\n",
      "Record count 100\n",
      "End Time 2017-06-01 22:46:02.866775\n"
     ]
    }
   ],
   "source": [
    "#Train model\n",
    "train_and_save_doc2vec_model(alldocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "5a1d7a9e-2eee-ebea-5ea8-97a0698fbb74",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_question_similarity_score(question1=\"\",question2=\"\"):\n",
    "    model_name = \"%s\" %(\"doc_2_vec_model1\")\n",
    "    model_saved_file = \"%s\" %(model_name)\n",
    "    model = gensim.models.doc2vec.Doc2Vec.load(model_saved_file)\n",
    "    \n",
    "    question_token1 = get_processed_text(question1)\n",
    "    tokenize_text1 = ' '.join(question_token1)\n",
    "    tokenize_text1 = gensim.utils.simple_preprocess(tokenize_text1)\n",
    "    infer_vector_of_question1 = model.infer_vector(tokenize_text1)\n",
    "    \n",
    "    \n",
    "    question_token2 = get_processed_text(question2)\n",
    "    tokenize_text2 = ' '.join(question_token2)\n",
    "    tokenize_text2 = gensim.utils.simple_preprocess(tokenize_text2)\n",
    "    infer_vector_of_question2 = model.infer_vector(tokenize_text2)\n",
    "    \n",
    "    similarity_score = 1\n",
    "    similarity_score = model.docvecs.most_similar([infer_vector_of_question1])\n",
    "   \n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "ccb128b3-22e8-6e48-e2c4-f011e4aa04a8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_id_list = []\n",
    "similarity_score_list = []\n",
    "df_test = pd.read_csv(\"data/train.csv\")\n",
    "for index,record in df_test.iterrows():\n",
    "    test_id = str(record[\"test_id\"])\n",
    "    question1 = str(record[\"question1\"])\n",
    "    question2 = str(record[\"question2\"])\n",
    "    similarity_score = get_question_similarity_score(question1,question2)\n",
    "    test_id_list.append(test_id)\n",
    "    similarity_score_list.append(similarity_score)\n",
    "\n",
    "print(similarity_score_list)\n",
    "submission = pd.DataFrame({\n",
    "\"test_id\": test_id_list,\n",
    "\"is_duplicate\": similarity_score_list})\n",
    "submission.to_csv('first_submition_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ac0871eb-9bad-78fe-ff0c-2f4713dea423"
   },
   "source": [
    "**Final Submition yet to come.Shorty i will update**"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 2,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
