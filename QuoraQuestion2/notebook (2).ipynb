{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Benchmark ##\n",
    "\n",
    "in this script we will explore perhaps the most trivial benchmark one can think of:\n",
    "\n",
    "extraction of \"bag of words\"/\"bag of letter sequences\" features followed by a logistic regression classifier\n",
    "\n",
    "\n",
    "----------\n",
    "**Note:** due to runtime limitation in the kernels platform one cannot really run this script to get a reasonable LB score, so **you will have to run it locally on your computer** and change some of the script parameters in CountVectorizer:\n",
    "\n",
    " - max_features = 300000 \n",
    " - min_df = 50\n",
    " - ngram_range = (1,10)\n",
    "\n",
    "On my laptop, this script with the above parameters runs for a little less than an hour, and achieves a LB score of 0.31675"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "matplotlib.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and show some samples of the data\n",
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "5  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6                                Should I buy tiago?   \n",
       "7                     How can I be a good geologist?   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...             1  \n",
       "6  What keeps childern active and far from phone ...             0  \n",
       "7          What should I do to be a great geologist?             1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% load train data\n",
    "\n",
    "trainDF = pd.read_csv('data/train.csv')\n",
    "trainDF = trainDF.dropna(how=\"any\").reset_index(drop=True)\n",
    "\n",
    "trainDF.ix[:7,3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionary and extract Bag of Words features from each question\n",
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% create dictionary and extract BOW features from questions\n",
    "featureExtractionStartTime = time.time()\n",
    "\n",
    "maxNumFeatures = 3000\n",
    "\n",
    "# bag of letter sequences (chars)\n",
    "BagOfWordsExtractor = CountVectorizer(max_df=0.999, min_df=50, max_features=maxNumFeatures, \n",
    "                                      analyzer='char', ngram_range=(2,3), \n",
    "                                      binary=True, lowercase=True)\n",
    "# bag of words\n",
    "#BagOfWordsExtractor = CountVectorizer(max_df=0.999, min_df=10, max_features=maxNumFeatures, \n",
    "#                                      analyzer='word', ngram_range=(1,6), stop_words='english', \n",
    "#                                      binary=True, lowercase=True)\n",
    "\n",
    "BagOfWordsExtractor.fit(pd.concat((trainDF.ix[:,'question1'],trainDF.ix[:,'question2'])).unique())\n",
    "\n",
    "trainQuestion1_BOW_rep = BagOfWordsExtractor.transform(trainDF.ix[:,'question1'])\n",
    "trainQuestion2_BOW_rep = BagOfWordsExtractor.transform(trainDF.ix[:,'question2'])\n",
    "lables = np.array(trainDF.ix[:,'is_duplicate'])\n",
    "\n",
    "featureExtractionDurationInMinutes = (time.time()-featureExtractionStartTime)/60.0\n",
    "print(\"feature extraction took %.2f minutes\" % (featureExtractionDurationInMinutes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation\n",
    "----------------\n",
    "\n",
    "combine the word representation to a single feature vector and run cross validation with logistic regression classifier\n",
    "\n",
    "**The first version of a possible feature is:**\n",
    "\n",
    " - take the value  \" 0\" if the particular letter sequence is either present or not present in both questions\n",
    " - take the value  \"-1\" if the particular letter sequence is present in one question but not the other question\n",
    "\n",
    "\n",
    "**The second version of a possible feature is:**\n",
    "\n",
    " - take the value  \" 1\" if the particular letter sequence is present in both questions\n",
    " - take the value  \" 0\" if the particular letter sequence is not present in any question\n",
    " - take the value \"-1\" if the particular letter sequence is present in one question but not the other question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% prefrom cross validation\n",
    "\n",
    "crossValidationStartTime = time.time()\n",
    "\n",
    "numCVSplits = 8\n",
    "numSplitsToBreakAfter = 2\n",
    "\n",
    "X = -(trainQuestion1_BOW_rep != trainQuestion2_BOW_rep).astype(int)\n",
    "#X = -(trainQuestion1_BOW_rep != trainQuestion2_BOW_rep).astype(int) + \\\n",
    "#      trainQuestion1_BOW_rep.multiply(trainQuestion2_BOW_rep)\n",
    "y = lables\n",
    "\n",
    "logisticRegressor = linear_model.LogisticRegression(C=0.1, solver='sag')\n",
    "\n",
    "logRegAccuracy = []\n",
    "logRegLogLoss = []\n",
    "logRegAUC = []\n",
    "\n",
    "print('---------------------------------------------')\n",
    "stratifiedCV = model_selection.StratifiedKFold(n_splits=numCVSplits, random_state=2)\n",
    "for k, (trainInds, validInds) in enumerate(stratifiedCV.split(X, y)):\n",
    "    foldTrainingStartTime = time.time()\n",
    "\n",
    "    X_train_cv = X[trainInds,:]\n",
    "    X_valid_cv = X[validInds,:]\n",
    "\n",
    "    y_train_cv = y[trainInds]\n",
    "    y_valid_cv = y[validInds]\n",
    "\n",
    "    logisticRegressor.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "    y_train_hat =  logisticRegressor.predict_proba(X_train_cv)[:,1]\n",
    "    y_valid_hat =  logisticRegressor.predict_proba(X_valid_cv)[:,1]\n",
    "\n",
    "    logRegAccuracy.append(accuracy_score(y_valid_cv, y_valid_hat > 0.5))\n",
    "    logRegLogLoss.append(log_loss(y_valid_cv, y_valid_hat))\n",
    "    logRegAUC.append(roc_auc_score(y_valid_cv, y_valid_hat))\n",
    "    \n",
    "    foldTrainingDurationInMinutes = (time.time()-foldTrainingStartTime)/60.0\n",
    "    print('fold %d took %.2f minutes: accuracy = %.3f, log loss = %.4f, AUC = %.3f' % (k+1,\n",
    "             foldTrainingDurationInMinutes, logRegAccuracy[-1],logRegLogLoss[-1],logRegAUC[-1]))\n",
    "\n",
    "    if (k+1) >= numSplitsToBreakAfter:\n",
    "        break\n",
    "\n",
    "\n",
    "crossValidationDurationInMinutes = (time.time()-crossValidationStartTime)/60.0\n",
    "\n",
    "print('---------------------------------------------')\n",
    "print('cross validation took %.2f minutes' % (crossValidationDurationInMinutes))\n",
    "print('mean CV: accuracy = %.3f, log loss = %.4f, AUC = %.3f' % (np.array(logRegAccuracy).mean(),\n",
    "                                                                 np.array(logRegLogLoss).mean(),\n",
    "                                                                 np.array(logRegAUC).mean()))\n",
    "print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show predictions and \"feature importance\"\n",
    "----------------------------------------\n",
    "\n",
    "Show prediction distribution on the validation set vs the ground truth \n",
    "\n",
    "Show the letter sequences that correspond to the largest positive coefficients and the largest negative (in absolute value) coefficients of the logistic regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% show prediction distribution and \"feature importance\"\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "plt.figure(); \n",
    "sns.kdeplot(y_valid_hat[y_valid_cv==0], shade=True, color=\"b\", bw=0.01)\n",
    "sns.kdeplot(y_valid_hat[y_valid_cv==1], shade=True, color=\"g\", bw=0.01)\n",
    "plt.legend(['non duplicate','duplicate'],fontsize=24)\n",
    "plt.title('Validation Accuracy = %.3f, Log Loss = %.4f, AUC = %.3f' %(logRegAccuracy[-1],\n",
    "                                                                      logRegLogLoss[-1],\n",
    "                                                                      logRegAUC[-1]))\n",
    "plt.xlabel('Prediction'); plt.ylabel('Probability Density'); plt.xlim(-0.01,1.01)\n",
    "\n",
    "\n",
    "numFeaturesToShow = 30\n",
    "\n",
    "sortedCoeffients = np.sort(logisticRegressor.coef_)[0]\n",
    "featureNames = BagOfWordsExtractor.get_feature_names()\n",
    "sortedFeatureNames = [featureNames[x] for x in list(np.argsort(logisticRegressor.coef_)[0])]\n",
    "\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['figure.figsize'] = (10,12)\n",
    "\n",
    "plt.figure()\n",
    "plt.suptitle('Feature Importance',fontsize=24)\n",
    "ax = plt.subplot(1,2,1); plt.title('top non duplicate predictors'); \n",
    "plt.xlabel('minus logistic regression coefficient')\n",
    "ax.barh(range(numFeaturesToShow), -sortedCoeffients[:numFeaturesToShow][::-1], align='center'); \n",
    "plt.ylim(-1,numFeaturesToShow); ax.set_yticks(range(numFeaturesToShow)); \n",
    "ax.set_yticklabels(sortedFeatureNames[:numFeaturesToShow][::-1],fontsize=20)\n",
    "\n",
    "ax = plt.subplot(1,2,2); plt.title('top duplicate predictors'); \n",
    "plt.xlabel('logistic regression coefficient')\n",
    "ax.barh(range(numFeaturesToShow), sortedCoeffients[-numFeaturesToShow:], align='center'); \n",
    "plt.ylim(-1,numFeaturesToShow); ax.set_yticks(range(numFeaturesToShow)); \n",
    "ax.set_yticklabels(sortedFeatureNames[-numFeaturesToShow:],fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on the full training data\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% train on full training data\n",
    "\n",
    "trainingStartTime = time.time()\n",
    "\n",
    "logisticRegressor = linear_model.LogisticRegression(C=0.1, solver='sag', \n",
    "                                                    class_weight={1: 0.46, 0: 1.32})\n",
    "logisticRegressor.fit(X, y)\n",
    "\n",
    "trainingDurationInMinutes = (time.time()-trainingStartTime)/60.0\n",
    "print('full training took %.2f minutes' % (trainingDurationInMinutes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features and make predictions on the test data\n",
    "------------------------------------------------------\n",
    "\n",
    "show the distribution of the validation predictions and the test predictions to make sure that the distributions are in fact different due to changing of the \"class weight\" argument in the logistic regression class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load test data, extract features and make predictions\n",
    "\n",
    "testPredictionStartTime = time.time()\n",
    "\n",
    "testDF = pd.read_csv('data/test.csv')\n",
    "testDF.ix[testDF['question1'].isnull(),['question1','question2']] = 'random empty question'\n",
    "testDF.ix[testDF['question2'].isnull(),['question1','question2']] = 'random empty question'\n",
    "\n",
    "testQuestion1_BOW_rep = BagOfWordsExtractor.transform(testDF.ix[:,'question1'])\n",
    "testQuestion2_BOW_rep = BagOfWordsExtractor.transform(testDF.ix[:,'question2'])\n",
    "\n",
    "X_test = -(testQuestion1_BOW_rep != testQuestion2_BOW_rep).astype(int)\n",
    "#X_test = -(testQuestion1_BOW_rep != testQuestion2_BOW_rep).astype(int) + \\\n",
    "#           testQuestion1_BOW_rep.multiply(testQuestion2_BOW_rep)\n",
    "\n",
    "#testPredictions = logisticRegressor.predict_proba(X_test)[:,1]\n",
    "\n",
    "# quick fix to avoid memory errors\n",
    "seperators= [750000,1500000]\n",
    "testPredictions1 = logisticRegressor.predict_proba(X_test[:seperators[0],:])[:,1]\n",
    "testPredictions2 = logisticRegressor.predict_proba(X_test[seperators[0]:seperators[1],:])[:,1]\n",
    "testPredictions3 = logisticRegressor.predict_proba(X_test[seperators[1]:,:])[:,1]\n",
    "testPredictions = np.hstack((testPredictions1,testPredictions2,testPredictions3))\n",
    "\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['figure.figsize'] = (9,9)\n",
    "\n",
    "plt.figure(); \n",
    "plt.subplot(2,1,1); sns.kdeplot(y_valid_hat, shade=True, color=\"b\", bw=0.01); \n",
    "plt.ylabel('Probability Density'); plt.xlim(-0.01,1.01)\n",
    "plt.title('mean valid prediction = ' + str(np.mean(y_valid_hat)))\n",
    "plt.subplot(2,1,2); sns.kdeplot(testPredictions, shade=True, color=\"b\", bw=0.01);\n",
    "plt.xlabel('Prediction'); plt.ylabel('Probability Density'); plt.xlim(-0.01,1.01)\n",
    "plt.title('mean test prediction = ' + str(np.mean(testPredictions)))\n",
    "\n",
    "testPredictionDurationInMinutes = (time.time()-testPredictionStartTime)/60.0\n",
    "print('predicting on test took %.2f minutes' % (testPredictionDurationInMinutes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Submission\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% create a submission\n",
    "\n",
    "submissionName = 'shallowBenchmark2'\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['test_id'] = testDF['test_id']\n",
    "submission['is_duplicate'] = testPredictions\n",
    "submission.to_csv(submissionName + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
